{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11d10d0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-29T12:33:21.027073Z",
     "iopub.status.busy": "2025-01-29T12:33:21.026760Z",
     "iopub.status.idle": "2025-01-29T12:45:42.111109Z",
     "shell.execute_reply": "2025-01-29T12:45:42.110150Z"
    },
    "papermill": {
     "duration": 741.088361,
     "end_time": "2025-01-29T12:45:42.112569",
     "exception": false,
     "start_time": "2025-01-29T12:33:21.024208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-4095d020c74a>:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully with modified state_dict and ready for testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping corrupted image /kaggle/input/vista-25/dataset/dataset/test/2643.jpg. Error: image file is truncated (3 bytes not processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (121554000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping corrupted image /kaggle/input/vista-25/dataset/dataset/test/103.jpg. Error: image file is truncated (6 bytes not processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping corrupted image /kaggle/input/vista-25/dataset/dataset/test/7711.jpg. Error: image file is truncated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (143040000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'mobilenetv2.csv' created with TTA.\n"
     ]
    }
   ],
   "source": [
    "# Necessary Imports\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Step 1: Set up the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 2: Define the Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [(os.path.join(root_dir, f), f) for f in os.listdir(root_dir)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, image_id = self.image_files[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, image_id\n",
    "        except (OSError, IOError) as e:\n",
    "            print(f\"Warning: Skipping corrupted image {img_path}. Error: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  # Return the next valid image\n",
    "\n",
    "# Step 3: Define Transforms\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Step 4: Load the Model\n",
    "state_dict_path = '/kaggle/input/mobilenet_v2/other/default/1/mobilenetv2_epoch_5.pth'  # Update with your model path\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(state_dict_path, map_location=device)\n",
    "\n",
    "# Remove 'module.' prefix from state_dict keys\n",
    "new_state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = mobilenet_v2(pretrained=False)\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)  # Adjust output classes\n",
    "model.load_state_dict(new_state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully with modified state_dict and ready for testing.\")\n",
    "\n",
    "# Step 5: Load Test Dataset\n",
    "test_dataset = CustomDataset(root_dir='/kaggle/input/vista-25/dataset/dataset/test', transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "# Step 6: TTA Function for Inference\n",
    "def tta_inference(model, dataloader, device):\n",
    "    model.eval()\n",
    "    tta_predictions = []\n",
    "    image_ids = []\n",
    "    with torch.no_grad():\n",
    "        for images, ids in dataloader:\n",
    "            if images is None:\n",
    "                continue\n",
    "            images = images.to(device)\n",
    "            outputs = []\n",
    "            for flip in [False, True]:  # Horizontal flip\n",
    "                augmented_images = torch.flip(images, dims=[3]) if flip else images\n",
    "                outputs.append(torch.softmax(model(augmented_images), dim=1))\n",
    "            averaged_outputs = torch.mean(torch.stack(outputs), dim=0)[:, 1]  # Probability of class 1\n",
    "            tta_predictions.extend(averaged_outputs.cpu().numpy())\n",
    "            image_ids.extend(ids)\n",
    "    return tta_predictions, image_ids\n",
    "\n",
    "# Step 7: Perform TTA Inference\n",
    "tta_predictions, image_ids = tta_inference(model, test_loader, device)\n",
    "\n",
    "# Step 8: Save Predictions to CSV\n",
    "submission = pd.DataFrame({'image_id': image_ids, 'label': tta_predictions})\n",
    "submission.to_csv('mobilenetv2 5.csv', index=False)\n",
    "print(\"Submission file 'mobilenetv2.csv' created with TTA.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10942507,
     "sourceId": 91830,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 230899,
     "modelInstanceId": 209206,
     "sourceId": 244878,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 230915,
     "modelInstanceId": 209223,
     "sourceId": 244897,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 746.11001,
   "end_time": "2025-01-29T12:45:44.540897",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-29T12:33:18.430887",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
